{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatador de mensagens do whatsapp do Boletim DOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "This script takes no arguments. \n",
    "\n",
    "It loads the DOU mat√©rias from section 2 from Google Sheets \n",
    "'Artigos novos do DOU e classifica√ß√£o' (that contains mat√©rias \n",
    "ranked by ML and then manually), selects those most relevant \n",
    "according to the manual classification in the spreadsheet (the \n",
    "criterium is hard coded below) and create a message almost ready \n",
    "(apart from minor corrections) to be posted on whatsapp. The \n",
    "message is opened in a text editor.\n",
    "\n",
    "Written by: Henrique S. Xavier, hsxavier@gmail.com, on 03/jul/2020.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date\n",
    "import warnings\n",
    "import subprocess\n",
    "import google.auth\n",
    "import os\n",
    "import csv\n",
    "\n",
    "### FUNCTIONS ###\n",
    "\n",
    "\n",
    "def bigquery_to_pandas(query, project='gabinete-compartilhado', credentials_file='/home/skems/gabinete/projetos/keys-configs/gabinete-compartilhado.json'):\n",
    "    \"\"\"\n",
    "    Run a query in Google BigQuery and return its results as a Pandas DataFrame. \n",
    "\n",
    "    Input\n",
    "    -----\n",
    "\n",
    "    query : str\n",
    "        The query to run in BigQuery, in standard SQL language.\n",
    "    project : str\n",
    "        \n",
    "    \n",
    "    Given a string 'query' with a query for Google BigQuery, returns a Pandas \n",
    "    dataframe with the results; The path to Google credentials and the name \n",
    "    of the Google project are hard-coded.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set authorization to access GBQ and gDrive:\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_file\n",
    "\n",
    "    \n",
    "    credentials, project = google.auth.default(scopes=[\n",
    "        'https://www.googleapis.com/auth/drive',\n",
    "        'https://www.googleapis.com/auth/bigquery',\n",
    "    ])\n",
    "    \n",
    "    return pd.read_gbq(query, project_id=project, dialect='standard', credentials=credentials)\n",
    "\n",
    "\n",
    "def load_data_from_local_or_bigquery(query, filename, force_bigquery=False, save_data=True, \n",
    "                                     project='gabinete-compartilhado', \n",
    "                                     credentials_file='/home/skems/gabinete/projetos/keys-configs/gabinete-compartilhado.json'):\n",
    "    \"\"\"\n",
    "    Loads data from local file if available or download it from BigQuery otherwise.\n",
    "    \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    query : str\n",
    "        The query to run in BigQuery.\n",
    "    \n",
    "    filename : str\n",
    "        The path to the file where to save the downloaded data and from where to load it.\n",
    "        \n",
    "    force_bigquery : bool (default False)\n",
    "        Whether to download data from BigQuery even if the local file exists.\n",
    "        \n",
    "    save_data : bool (default True)\n",
    "        Wheter to save downloaded data to local file or not.\n",
    "        \n",
    "    project : str (default 'gabinete-compartilhado')\n",
    "        The GCP project where to run BigQuery.\n",
    "        \n",
    "    credentials_file : str (default path to 'gabinete-compartilhado.json')\n",
    "        The path to the JSON file containing the credentials used to access GCP.\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    df : Pandas DataFrame\n",
    "        The data either loaded from `filename` or retrieved through `query`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download data from BigQuery and save it to local file:\n",
    "    if os.path.isfile(filename) == False or force_bigquery == True:\n",
    "        print('Loading data from BigQuery...')\n",
    "        df = bigquery_to_pandas(query, project, credentials_file)\n",
    "        if save_data:\n",
    "            print('Saving data to local file...')\n",
    "            df.to_csv(filename, quoting=csv.QUOTE_ALL, index=False)\n",
    "    \n",
    "    # Load data from local file:\n",
    "    else:\n",
    "        print('Loading data from local file...')\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def split_acts(text_series, splitter):\n",
    "    \"\"\"\n",
    "    Split an article into acts over people (nomear, exonerar, etc).\n",
    "    The preface is thrown away. Each act gets its own row, with \n",
    "    the same index as the original text.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    text_series : Pandas Series of str\n",
    "        The text of the mat√©rias to be split into acts, including \n",
    "        a token that identifies where the split should be made.\n",
    "        \n",
    "    splitter : str\n",
    "        A token that identifies where the string should be split.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    series : Pandas Series of str\n",
    "        The nomear/exonerar/etc acts, one per row, with \n",
    "        same index as original text (so indices might repeat).\n",
    "    \"\"\"\n",
    "    text_segments = text_series.str.split(splitter)\n",
    "    acts = text_segments.apply(lambda text_list: text_list[1:])\n",
    "    series = acts.explode()\n",
    "    return series\n",
    "    \n",
    "\n",
    "def trim_act_tail(text_series):\n",
    "    \"\"\"\n",
    "    Remove from the end of each nomear/exonerar/etc act\n",
    "    in `text_series` (Pandas Series) the preface of the \n",
    "    following act, which is unnecessary information for the acts.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Considerar . final do ato, a menos que acompanhe n√∫meros ou * em seguida ou come√ße com \"art\" ou \" n\".\n",
    "    sentence_end_regex = r'(?<!(?:[Rr][Tt]| [Nn]))\\.(?:[^\\d*]|$)'\n",
    "    trimmed_act = text_series.str.split(sentence_end_regex).apply(lambda arr: arr[0]) + '.'\n",
    "    \n",
    "    return trimmed_act\n",
    "\n",
    "\n",
    "def isolate_acts(text_series, act_regex):\n",
    "    \"\"\"\n",
    "    Isolate each nomear/exonerar/etc act in each row of \n",
    "    `text_series` (Pandas Series) into a different row, \n",
    "    maintaining the same indices for the acts coming from \n",
    "    the same original `text_series` row. The acts are detected \n",
    "    by starting with the `act_regex`.\n",
    "    \n",
    "    Returns a Pandas Series.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mark splitting points:\n",
    "    act_splitter = 'XXDIVXX'\n",
    "    texts_with_splitter = text_series.str.replace(act_regex, act_splitter + r'\\1', case=False)\n",
    "    # Split acts:\n",
    "    act_series = split_acts(texts_with_splitter, act_splitter)\n",
    "    \n",
    "    # Trim garbage from the end of the acts:\n",
    "    cleaned_act_series = trim_act_tail(act_series)\n",
    "    \n",
    "    return cleaned_act_series\n",
    "\n",
    "\n",
    "def remove_pattern(text_series, regex):\n",
    "    \"\"\"\n",
    "    Remove a `regex` from all rows in a `text_series`.\n",
    "    \"\"\"\n",
    "    cleaned_series = text_series.str.replace(regex, '', case=False)\n",
    "    return cleaned_series\n",
    "\n",
    "\n",
    "def remove_siape(text_series):\n",
    "    \"\"\"\n",
    "    Remove SIAPE code (and related terms) from all rows in \n",
    "    `text_series`.\n",
    "    \"\"\"\n",
    "    siape_regex = r',?\\s*?(?:(?:matr[√≠i]cula)?\\s*siape(?:cad)?|matr[√≠i]cula)\\s*?n?.?\\s*?(\\d{5,7}),?'\n",
    "    return remove_pattern(text_series, siape_regex)\n",
    "\n",
    "\n",
    "def remove_cpf(text_series):\n",
    "    \"\"\"\n",
    "    Remove CPF number (and related terms) from all rows in \n",
    "    `text_series`.\n",
    "    \"\"\"\n",
    "    cpf_regex = r',?\\s*?cpf\\s*?n?\\.?.?\\s*?([\\d.*-]{14,18}),?'\n",
    "    return remove_pattern(text_series, cpf_regex)\n",
    "\n",
    "\n",
    "def remove_no(text_series):\n",
    "    no_regex = r',?\\s*?c[o√≥]digo\\s*?n.? ?[\\.\\d]{5,7},?'\n",
    "    return remove_pattern(text_series, no_regex)\n",
    "\n",
    "\n",
    "def remove_processo(text_series):\n",
    "    processo_regex = r'(?:,?\\s*?conforme\\s*?|[\\s\\-.]*?)\\(?Processo\\s*?(?:SEI)?\\s*?n?.?\\s*?[\\d.\\-/]{15,20}\\)?'\n",
    "    return remove_pattern(text_series, processo_regex)\n",
    "\n",
    "\n",
    "def fix_verbs(text_series):\n",
    "    \"\"\"\n",
    "    Replace infinitive of main verbs of the acts (nomear, exonerar, etc.)\n",
    "    by present tense.\n",
    "    \"\"\"\n",
    "    clean_series = text_series.copy()\n",
    "    clean_series = clean_series.str.replace(r'nomear ?(,?)\\s*', r'Nomeia\\1 ', case=False)\n",
    "    clean_series = clean_series.str.replace(r'exonerar ?(,?)\\s*', r'Exonera\\1 ', case=False)\n",
    "    clean_series = clean_series.str.replace(r'designar ?(,?)\\s*', r'Designa\\1 ', case=False)\n",
    "    clean_series = clean_series.str.replace(r'dispensar ?(,?)\\s*', r'Dispensa\\1 ', case=False)\n",
    "    return clean_series\n",
    "\n",
    "\n",
    "def remove_preamble(text_series):\n",
    "    \"\"\"\n",
    "    Remove preamble (whose end is identified by 'resolve:')\n",
    "    from all rows in `text_series`.\n",
    "    \"\"\"\n",
    "    preamble_regex = '^.*?resolve:\\s*'\n",
    "    return remove_pattern(text_series, preamble_regex)\n",
    "\n",
    "\n",
    "def filter_low_cargos(text_series):\n",
    "    \"\"\"\n",
    "    Remove rows from `text_series` that contains low cargos \n",
    "    and, also, do not contain high cargos (all hard-coded).\n",
    "    \"\"\"\n",
    "    low_cargo_regex  = '(?:(?:das|fcp?e)[ -]*?[0123]{3}\\.[1-3]|cge[ -]+?(iii|iv|v)(?:\\W|$))'\n",
    "    high_cargo_regex = '(?:(?:das|fcp?e)[ -]*?[0123]{3}\\.[4-6]|cge[ -]+?(i|ii)(?:\\W|$))'\n",
    "    filtered = text_series.loc[~((text_series.str.contains(low_cargo_regex, case=False)) & \n",
    "                                ~(text_series.str.contains(high_cargo_regex, case=False)))]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def standardize_cargos(text_series):    \n",
    "    \"\"\"\n",
    "    Standardize and simplify parts of text in `text_series` \n",
    "    (Pandas Series) describing cargos.\n",
    "    \"\"\"\n",
    "    prefix_regex = [('DAS ',    r',?\\s*?(?:c[√≥o]digo)?\\s*?das[ -]*?[0123]{3}\\.([1-6]),?'), \n",
    "                    ('CA ',     r',?\\s*?(?:c[√≥o]digo)?\\s*?ca[ -]+?(i{1,4})(?:\\W|$),?'),\n",
    "                    ('CA-APO ', r',?\\s*?(?:c[√≥o]digo)?\\s*?ca-apo[ -]*?([12]),?'),\n",
    "                    ('',        r',?\\s*?(?:c[√≥o]digo)?\\s*?\\W(CDT)\\W,?'),\n",
    "                    ('CCD ',    r',?\\s*?(?:c[√≥o]digo)?\\s*?ccd[ -]+?(i{1,3})(?:\\W|$),?'),\n",
    "                    ('CGE ',    r',?\\s*?(?:c[√≥o]digo)?\\s*?cge[ -]+?(i{1,3})(?:\\W|$),?'),\n",
    "                    ('',        r',?\\s*?(?:c[√≥o]digo)?\\s*?(CPAGLO),?'),\n",
    "                    ('',        r',?\\s*?(?:c[√≥o]digo)?\\s*?\\W(CSP)(?:\\W|$),?'),\n",
    "                    ('',        r',?\\s*?(?:c[√≥o]digo)?\\s*?\\W(CSU)(?:\\W|$),?'),\n",
    "                    ('CD ',     r',?\\s*?(?:c[√≥o]digo)?\\s*?\\Wcd(?:[ -]*?|\\.)([123])(?:\\W|$),?'),\n",
    "                    ('',        r',?\\s*?(?:c[√≥o]digo)?\\s*?\\W(NE)(?:\\W|$),?'),\n",
    "                    ('CETG ',   r',?\\s*?(?:c[√≥o]digo)?\\s*?cetg[ -]*?(iv|v|vi|vii)(?:\\W|$),?'), \n",
    "                    ('FDS ',    r',?\\s*?(?:c[√≥o]digo)?\\s*?\\Wfds[ -]*?(1)(?:\\W|$),?'),\n",
    "                    ('FCPE ',   r',?\\s*?(?:c[√≥o]digo)?\\s*?fc?pe[ -]*?[0-9]{3}\\.([1-5]),?'),\n",
    "                    ('',        '(natureza especial)'),\n",
    "                    ('CNE ',    r',?\\s*?(?:c[√≥o]digo)?\\s*?cne[ -]*?([0-9]{2}),?')]\n",
    "    \n",
    "    new_text_series = text_series.copy()\n",
    "    for prefix, regex in prefix_regex:\n",
    "        new_text_series = new_text_series.str.replace(regex, ' (' + prefix + r'\\1)', case=False)\n",
    "\n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def add_label_to_df(df, orgao_label_df, lookup_col='orgao'):\n",
    "    \"\"\"\n",
    "    Modify `df` (Pandas DataFrame) in place by adding a 'label'\n",
    "    column that translates regex patterns looked for in `df` column \n",
    "    `lookup_col` (default 'orgao') to labels. The regex patterns and the \n",
    "    respective labels are stored in columns 'regex' and 'label 'from \n",
    "    `orgao_label_df` (Pandas DataFrame).\n",
    "    \"\"\"\n",
    "    df['label'] = None\n",
    "\n",
    "    for i in range(len(orgao_label)):\n",
    "\n",
    "        # Get one regex-label pair:\n",
    "        regex = orgao_label_df.loc[i, 'regex']\n",
    "        label = orgao_label_df.loc[i, 'label']\n",
    "\n",
    "        df.loc[(df[lookup_col].str.contains(regex)) & (df['label'].isnull()), 'label'] = label\n",
    "    \n",
    "    # Default:\n",
    "    df['label'].fillna('Outros')\n",
    "\n",
    "    \n",
    "def act_importance(text):\n",
    "    \"\"\"\n",
    "    Return a number representing the importance of the cargo found \n",
    "    in `text` (str).\n",
    "    \n",
    "    Note that the cargos in `text` must be standardized to the \n",
    "    hard-coded tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_importance = [('(DAS 6)', 6), ('(DAS 5)', 5), ('(DAS 4)', 4), ('(FCPE 5)', 5), ('(FCPE 4)', 4),\n",
    "                      ('(CGE I)', 5), ('(CGE II)', 4)]\n",
    "    \n",
    "    for tag, importance in tag_importance:\n",
    "        if text.find(tag) != -1:\n",
    "            return importance\n",
    "    \n",
    "    return 0   \n",
    "\n",
    "\n",
    "def remove_nomeia_cargo_preamble(text_series):\n",
    "    \"\"\"\n",
    "    Remove the preamble for a cargo/fun√ß√£o from every \n",
    "    row in a `text_series`.\n",
    "    \n",
    "    (e.g. 'para exercer o cargo de')\n",
    "    \"\"\"\n",
    "    # Regex of the beginning:\n",
    "    enter_cargo_preamble_0 = ',?\\s*?para\\s*?(?:exercer|ocupar)\\s*?'\n",
    "    # Regex for cargo/fun√ß√£o:\n",
    "    enter_cargo_comissao   = 'o?\\s*?cargo\\s*?(?:em\\s*?comiss[a√£]o|comissionado)?'\n",
    "    enter_cargo_funcao     = 'a?\\s*?fun[c√ß][a√£]o(?:\\s*?comissionada)?(?:\\s*?do\\s*?poder\\s*?executivo)?'\n",
    "    # Regex for the final part:\n",
    "    enter_cargo_preamble_1 = '\\s*?de'\n",
    "    # Full regex:\n",
    "    preamble_regex = enter_cargo_preamble_0 + '(?:' + enter_cargo_funcao + '|' + enter_cargo_comissao + ')' \\\n",
    "                   + enter_cargo_preamble_1\n",
    "    \n",
    "    # Remove preamble:\n",
    "    new_text_series = text_series.str.replace('(' + preamble_regex + ')', '', case=False)\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def simplify_exonera_cargo_preamble(text_series):\n",
    "    \"\"\"\n",
    "    Simplify preamble of a cargo/fun√ß√£o in the case of \n",
    "    a exonera√ß√£o/dispensa.\n",
    "    \n",
    "    (e.g.: 'do cargo comissioado de' -> 'do cargo de')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regexes:\n",
    "    exit_cargo_preamble  = '(do\\s*?cargo\\s*?(?:em\\s*?comiss√£o|comissionado)?\\s*?de)'\n",
    "    exit_funcao_preamble = '(da\\s*?fun√ß√£o\\s*?comissionada\\s*?(?:do\\s*?poder\\s*?executivo)?\\s*?de)'\n",
    "    \n",
    "    # Transform text series:\n",
    "    new_text_series = text_series.copy()\n",
    "    new_text_series = new_text_series.str.replace(exit_cargo_preamble, 'do cargo de', case=False)\n",
    "    new_text_series = new_text_series.str.replace(exit_funcao_preamble, 'da fun√ß√£o de', case=False)\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def simplify_cargo_preamble(text_series):\n",
    "    \"\"\"\n",
    "    Simplify preambles like 'para exercer o cargo de' and\n",
    "    'da fun√ß√£o comissionada do poder executivo' in `text_series`.\n",
    "    \"\"\"\n",
    "    new_text_series = text_series.copy()\n",
    "    \n",
    "    # Location of exonera/dispensa and nomeia/designa acts:\n",
    "    exonera_cases = new_text_series.str.contains('^(?:exonera|dispensa)', case=False)\n",
    "    nomeia_cases  = new_text_series.str.contains('^(?:nomeia|designa)', case=False)\n",
    "    \n",
    "    # Replace large texts for shorter ones:\n",
    "    new_text_series.loc[exonera_cases] = simplify_exonera_cargo_preamble(new_text_series.loc[exonera_cases])\n",
    "    new_text_series.loc[nomeia_cases]  = remove_nomeia_cargo_preamble(new_text_series.loc[nomeia_cases])\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def truncate_text(text, n_chars=400):\n",
    "    \"\"\"\n",
    "    If `text` (str) is longer than `n_chars` (int), return \n",
    "    the first `n_chars` characters followed by '...'; \n",
    "    otherwise, return `text`.\n",
    "    \"\"\"\n",
    "    text_len = len(text)\n",
    "    if text_len <= n_chars:\n",
    "        return text\n",
    "    else:\n",
    "        return text[:n_chars] + '...'\n",
    "    \n",
    "\n",
    "def prep_orgao_regex(name, acronym):\n",
    "    \"\"\"\n",
    "    Given a org√£o `name` and its `acronym`, return a regex\n",
    "    that detects the name, possibly followed by the acronym \n",
    "    with possible variations in terms of spacing and other \n",
    "    formatting.\n",
    "    \"\"\"\n",
    "    regex = name.replace(' ', r'\\s*?') + '[\\s-]*(?:\\(?' + acronym + '\\)?)?'\n",
    "    return regex\n",
    "\n",
    "\n",
    "def name_to_sigla(text_series):\n",
    "    \"\"\"\n",
    "    Replace long reference of a org√£o (name + possible acronym)\n",
    "    by its acronym in a `text_series`. All org√£os are hard-coded. \n",
    "    \"\"\"\n",
    "\n",
    "    # Hard-coded acronyms and names of √≥rg√£os:\n",
    "    sigla_list = ['FNDE', 'IBAMA', 'ICMBio', 'INCRA', 'FUNAI', 'CAPES', 'INEP', 'CNPq', 'ABIN']\n",
    "    orgao_list = ['Fundo Nacional de Desenvolvimento da Educa[c√ß][a√£]o',\n",
    "                  'Instituto Brasileiro do Meio Ambiente e dos Recursos Naturais Renov[a√°]veis',\n",
    "                  'Instituto Chico Mendes de Conserva[c√ß][a√£]o da Biodiversidade',\n",
    "                  'Instituto Nacional de Coloniza[c√ß][a√£]o e Reforma Agr[a√°]ria',\n",
    "                  'Funda[c√ß][a√£]o Nacional do [I√ç]ndio',\n",
    "                  'Coordena[c√ß][a√£]o de Aperfei[c√ß]oamento de Pessoal de N[i√≠]vel Superior',\n",
    "                  'Instituto Nacional de Estudos e Pesquisas Educacionais An[i√≠]sio Teixeira',\n",
    "                  'Conselho Nacional de Desenvolvimento Cient[i√≠]fico e Tecnol[o√≥]gico',\n",
    "                  'Ag[e√™]ncia Brasileira de Intelig[e√™]ncia']\n",
    "    # Create robust regexes out of name and acronym:\n",
    "    regex_list = [prep_orgao_regex(name, acronym) for name, acronym in zip(orgao_list, sigla_list)]\n",
    "    \n",
    "    new_text_series = text_series.copy()\n",
    "    for regex, sigla in zip(regex_list, sigla_list):\n",
    "        new_text_series = new_text_series.str.replace(regex, sigla, case=False)\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def remove_dates(text_series):\n",
    "    \"\"\"\n",
    "    Remove references to dates that start with 'a partir de'\n",
    "    or 'a contar de'.\n",
    "    \"\"\"\n",
    "    # Preparing regex:\n",
    "    mes_list = ['janeiro', 'fevereiro', 'mar[c√ß]o', 'abril', 'maio', 'junho', 'julho', 'agosto', 'setembro', \n",
    "                'outubro', 'novembro', 'dezembro']\n",
    "    mes_regex = '(?:' + '|'.join(mes_list) + ')'\n",
    "    data_regex = r',? a (?:partir|contar) de (?:\\d{1,2}.? de ' + mes_regex + ' de (?:20|19)\\d{2}|\\d{1,2}/\\d{1,2}/\\d{4}),?'\n",
    "    data_regex = data_regex.replace(' ', '\\s*?')\n",
    "    \n",
    "    new_text_series = text_series.str.replace(data_regex, '', case=False)\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def assign_emoji(act_text):\n",
    "    \"\"\"\n",
    "    Returns a certain hard-coded emoji given a hard-coded regex \n",
    "    found in `act_text` (str).\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of regexes and emojis. The list is ordered by preference:\n",
    "    regex_emoji = [('substitu', 'üéí'), (r'pol[i√≠]cia\\s*?(?:rodovi[a√°]ria)?\\s*?federal', 'üëÆüèª'),\n",
    "               (r'\\(DAS 6\\)', 'üëë'), (r'\\((?:DAS|FCPE) 5\\)', 'üé©'), (r'\\((?:DAS|FCPE) 4\\)', 'üß¢'),  \n",
    "               (r'\\((?:CA|CGE) I{1,3}\\)', 'üíº'), (r'\\(CDT\\)', 'üëì'), \n",
    "               (r'(?:grupo de trabalho|comit√™|conselho)', 'üí¨')]\n",
    "    \n",
    "    # Look for patterns:\n",
    "    for regex, emoji in regex_emoji:\n",
    "        if re.search(regex, act_text, flags=re.IGNORECASE) != None:\n",
    "            return emoji\n",
    "    \n",
    "    # Default return:\n",
    "    return '‚ñ™Ô∏è'\n",
    "\n",
    "\n",
    "def prepare_with_acts(materia_series, act_regex):\n",
    "    \"\"\"\n",
    "    Process `materia_series` (Pandas Series) of mat√©rias from DOU that \n",
    "    contains the pattern `act_regex`. Those are assumed to be \n",
    "    standard nomea√ß√µes/exonera√ß√µes/designa√ß√µes/dispensas.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    cleaned_acts : Pandas Series\n",
    "        The individual acts present in mat√©rias, with low cargos\n",
    "        removed and with its texts cleaned. Acts (each \n",
    "        nomea√ß√£o/exonera√ß√£o) in the same mat√©ria share the same \n",
    "        index as the original mat√©ria in `materia_series`.\n",
    "    \"\"\"\n",
    "    # Isolate each act in a different row:\n",
    "    raw_acts = isolate_acts(materia_series, act_regex)\n",
    "\n",
    "    # Filter acts containing only low cargos:\n",
    "    filtered_acts = filter_low_cargos(raw_acts)\n",
    "\n",
    "    # Remove unwanted information:\n",
    "    cleaned_acts  = filtered_acts\n",
    "    cleaned_acts  = remove_siape(cleaned_acts)\n",
    "    cleaned_acts  = remove_cpf(cleaned_acts)\n",
    "    cleaned_acts  = remove_no(cleaned_acts)\n",
    "    cleaned_acts  = remove_processo(cleaned_acts)\n",
    "    \n",
    "    # Clean text:\n",
    "    cleaned_acts  = fix_verbs(cleaned_acts)\n",
    "    cleaned_acts  = standardize_cargos(cleaned_acts)\n",
    "    cleaned_acts  = simplify_cargo_preamble(cleaned_acts)\n",
    "    cleaned_acts  = name_to_sigla(cleaned_acts)\n",
    "    cleaned_acts  = remove_dates(cleaned_acts)\n",
    "    \n",
    "    return cleaned_acts\n",
    "\n",
    "\n",
    "def prepare_no_acts(materia_series):\n",
    "    \"\"\"\n",
    "    Clean `materia_series` (Pandas Series) of mat√©rias from DOU \n",
    "    that do not contain typical verbs of nomea√ß√£o/exonera√ß√£o, etc.\n",
    "    \"\"\"\n",
    "    cleaned_non_acts = materia_series\n",
    "    cleaned_non_acts = remove_preamble(cleaned_non_acts)\n",
    "    cleaned_non_acts = cleaned_non_acts.apply(truncate_text)\n",
    "    \n",
    "    return cleaned_non_acts\n",
    "\n",
    "\n",
    "def sort_orgaos_by_acts_importance(message_df, orgao_importance):\n",
    "    \"\"\"\n",
    "    Define the order of the org√£os in the message, according to \n",
    "    the relevance of the acts.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    message_df : Pandas DataFrame\n",
    "        A DataFrame containing the section in which the acts \n",
    "        will be published and their importance.\n",
    "        \n",
    "    orgao_importance : Pandas Series\n",
    "        A series whose indices are the sections (labels for org√£os)\n",
    "        and the values are the label's importances, for breaking \n",
    "        ties.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    ordered_sections : array\n",
    "        The org√£os labels sorted as they should appear in the message.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute stats for each orgao (message's sections):\n",
    "    sections_groupby_importance = message_df.groupby('section')['importance']\n",
    "    sections_importance_sum = sections_groupby_importance.sum()\n",
    "    sections_importance_max = sections_groupby_importance.max()\n",
    "    \n",
    "    # Build DataFrame with the section's stats:\n",
    "    sections_ranking_df = pd.DataFrame()\n",
    "    sections_ranking_df['max'] = sections_importance_max\n",
    "    sections_ranking_df['sum'] = sections_importance_sum\n",
    "    sections_ranking_df = sections_ranking_df.join(orgao_importance, how='left')\n",
    "\n",
    "    # Order the sections:\n",
    "    ordered_sections = sections_ranking_df.sort_values(['max','sum', 'importance'], ascending=False).index.values\n",
    "    \n",
    "    return ordered_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading org√£o-label table...\n",
      "Loading data from BigQuery...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  6.69rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to local file...\n",
      "# mat√©rias (all): 9\n",
      "Processing the mat√©rias...\n",
      "# mat√©rias (containing act verbs): 8\n",
      "# mat√©rias (without act verbs): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### MAIN CODE ###\n",
    "\n",
    "\n",
    "### Hard-coded stuff:\n",
    "\n",
    "orgao_label_file = 'data/correspondencia_orgao_label_DOU_2.csv'\n",
    "todays_dou_data  = 'temp/daily_ranked_dou_2_set.csv'\n",
    "zap_group_link   = 'https://chat.whatsapp.com/JUwrwHaDpnBK6WnAgMTLXV'\n",
    "#zap_group_link   = 'https://chat.whatsapp.com/GJGfNBzcZZT9xuueZUBbTU'\n",
    "post_file_prefix = 'posts/dou_2_'\n",
    "text_editor      = 'gedit'\n",
    "\n",
    "prod_query = \"\"\"\n",
    "SELECT relevancia, identifica, secao, edicao, data_pub, orgao, ementa, resumo, fulltext, assina, cargo, url \n",
    "FROM `gabinete-compartilhado.executivo_federal_dou.sheets_classificacao_secao_2`\n",
    "WHERE relevancia IS NOT NULL\n",
    "AND   relevancia >= 3\n",
    "\"\"\"\n",
    "\n",
    "### Load data:\n",
    "\n",
    "# Table that translates orgao to message topic:\n",
    "print('Loading org√£o-label table...')\n",
    "orgao_label = pd.read_csv(orgao_label_file)\n",
    "\n",
    "# Download today's ranked DOU (section 2) materias:\n",
    "test_set = load_data_from_local_or_bigquery(prod_query, todays_dou_data, force_bigquery=True)\n",
    "print('# mat√©rias (all):', len(test_set))\n",
    "\n",
    "\n",
    "### Prepare the data:\n",
    "\n",
    "print('Processing the mat√©rias...')\n",
    "\n",
    "# Add label tag to all texts:\n",
    "add_label_to_df(test_set, orgao_label)\n",
    "\n",
    "# Use regex to detect typical act verbs:\n",
    "enter_regex   = r'nomear|designar'\n",
    "exit_regex    = r'exonerar|dispensar'\n",
    "flexing_regex = r'(?!(?:am|√°|√£o|em))'\n",
    "act_regex     = r'(' + enter_regex + '|' + exit_regex + ')' + flexing_regex\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    # Get articles with default act detectors:\n",
    "    with_act_regex_df = test_set.loc[test_set.fulltext.str.contains(act_regex, case=False)]\n",
    "    print('# mat√©rias (containing act verbs):', len(with_act_regex_df))\n",
    "    # Get articles without default act detectors:\n",
    "    no_act_regex_df   = test_set.loc[~test_set.fulltext.str.contains(act_regex, case=False)]\n",
    "    print('# mat√©rias (without act verbs):', len(no_act_regex_df))\n",
    "\n",
    "    # Clean acts for posting:\n",
    "    cleaned_with_acts = prepare_with_acts(with_act_regex_df['fulltext'], act_regex)\n",
    "    cleaned_no_acts   = prepare_no_acts(no_act_regex_df['fulltext'])\n",
    "    # Join both kinds of mat√©rias:\n",
    "    cleaned_all = pd.concat([cleaned_with_acts, cleaned_no_acts], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the post...\n",
      "Writing post...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prepare the message:\n",
    "\n",
    "print('Preparing the post...')\n",
    "\n",
    "# Build zap message DataFrame:\n",
    "message_df = pd.DataFrame()\n",
    "message_df['text']       = cleaned_all\n",
    "message_df['importance'] = cleaned_all.apply(act_importance)\n",
    "message_df['section']    = test_set['label'][cleaned_all.index]\n",
    "message_df['url']        = test_set['url'][cleaned_all.index]\n",
    "message_df = message_df.reset_index(drop=True)\n",
    "\n",
    "# Prepare the order in which the org√£os will appear in the message:\n",
    "ordered_sections = sort_orgaos_by_acts_importance(message_df, orgao_label.set_index('label')['importance'])\n",
    "\n",
    "\n",
    "### Print the message:\n",
    "\n",
    "print('Writing post...')\n",
    "\n",
    "filename = post_file_prefix + date.today().strftime('%Y-%m-%d') + '.txt'\n",
    "with open(filename, 'w') as f:\n",
    "\n",
    "    # Header:\n",
    "    today = date.today().strftime(' (%d/%m)')\n",
    "    f.write('‚ôüÔ∏è *Altera√ß√µes em cargos altos' + today + '* ‚ôüÔ∏è\\n\\n')\n",
    "\n",
    "    # Loop over org√£os:\n",
    "    for s in ordered_sections:\n",
    "        f.write('*' + s + '*\\n\\n')\n",
    "\n",
    "        # Select acts from this section:\n",
    "        section_acts = message_df.loc[message_df['section'] == s].sort_values('importance', ascending=False)\n",
    "        for t, u in zip(section_acts['text'].values, section_acts['url'].values):\n",
    "            # Print message:\n",
    "            e = assign_emoji(t)\n",
    "            f.write(e + ' ' + t + '\\n' + u + '\\n\\n')\n",
    "\n",
    "            # Footnote:        \n",
    "    f.write('*Gabinete Compartilhado Acredito*\\n_Para se inscrever no boletim, acesse o link:_\\n' + zap_group_link)\n",
    "\n",
    "\n",
    "### Open text editor:\n",
    "\n",
    "subprocess.call([text_editor, filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
