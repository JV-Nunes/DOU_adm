{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatador de mensagens do whatsapp do Boletim DOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "This script takes no arguments. \n",
    "\n",
    "It loads the DOU matérias from section 2 from Google Sheets \n",
    "'Artigos novos do DOU e classificação' (that contains matérias \n",
    "ranked by ML and then manually), selects those most relevant \n",
    "according to the manual classification in the spreadsheet (the \n",
    "criterium is hard coded below) and create a message almost ready \n",
    "(apart from minor corrections) to be posted on whatsapp. The \n",
    "message is opened in a text editor.\n",
    "\n",
    "Written by: Henrique S. Xavier, hsxavier@gmail.com, on 03/jul/2020.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date\n",
    "import warnings\n",
    "import subprocess\n",
    "import google.auth\n",
    "import os\n",
    "import csv\n",
    "\n",
    "### FUNCTIONS ###\n",
    "\n",
    "\n",
    "def bigquery_to_pandas(query, project='gabinete-compartilhado', credentials_file='/home/skems/gabinete/projetos/keys-configs/gabinete-compartilhado.json'):\n",
    "    \"\"\"\n",
    "    Run a query in Google BigQuery and return its results as a Pandas DataFrame. \n",
    "\n",
    "    Input\n",
    "    -----\n",
    "\n",
    "    query : str\n",
    "        The query to run in BigQuery, in standard SQL language.\n",
    "    project : str\n",
    "        \n",
    "    \n",
    "    Given a string 'query' with a query for Google BigQuery, returns a Pandas \n",
    "    dataframe with the results; The path to Google credentials and the name \n",
    "    of the Google project are hard-coded.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set authorization to access GBQ and gDrive:\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_file\n",
    "\n",
    "    \n",
    "    credentials, project = google.auth.default(scopes=[\n",
    "        'https://www.googleapis.com/auth/drive',\n",
    "        'https://www.googleapis.com/auth/bigquery',\n",
    "    ])\n",
    "    \n",
    "    return pd.read_gbq(query, project_id=project, dialect='standard', credentials=credentials)\n",
    "\n",
    "\n",
    "def load_data_from_local_or_bigquery(query, filename, force_bigquery=False, save_data=True, \n",
    "                                     project='gabinete-compartilhado', \n",
    "                                     credentials_file='/home/skems/gabinete/projetos/keys-configs/gabinete-compartilhado.json'):\n",
    "    \"\"\"\n",
    "    Loads data from local file if available or download it from BigQuery otherwise.\n",
    "    \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    query : str\n",
    "        The query to run in BigQuery.\n",
    "    \n",
    "    filename : str\n",
    "        The path to the file where to save the downloaded data and from where to load it.\n",
    "        \n",
    "    force_bigquery : bool (default False)\n",
    "        Whether to download data from BigQuery even if the local file exists.\n",
    "        \n",
    "    save_data : bool (default True)\n",
    "        Wheter to save downloaded data to local file or not.\n",
    "        \n",
    "    project : str (default 'gabinete-compartilhado')\n",
    "        The GCP project where to run BigQuery.\n",
    "        \n",
    "    credentials_file : str (default path to 'gabinete-compartilhado.json')\n",
    "        The path to the JSON file containing the credentials used to access GCP.\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    df : Pandas DataFrame\n",
    "        The data either loaded from `filename` or retrieved through `query`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download data from BigQuery and save it to local file:\n",
    "    if os.path.isfile(filename) == False or force_bigquery == True:\n",
    "        print('Loading data from BigQuery...')\n",
    "        df = bigquery_to_pandas(query, project, credentials_file)\n",
    "        if save_data:\n",
    "            print('Saving data to local file...')\n",
    "            df.to_csv(filename, quoting=csv.QUOTE_ALL, index=False)\n",
    "    \n",
    "    # Load data from local file:\n",
    "    else:\n",
    "        print('Loading data from local file...')\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def split_acts(text_series, splitter):\n",
    "    \"\"\"\n",
    "    Split an article into acts over people (nomear, exonerar, etc).\n",
    "    The preface is thrown away. Each act gets its own row, with \n",
    "    the same index as the original text.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    text_series : Pandas Series of str\n",
    "        The text of the matérias to be split into acts, including \n",
    "        a token that identifies where the split should be made.\n",
    "        \n",
    "    splitter : str\n",
    "        A token that identifies where the string should be split.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    series : Pandas Series of str\n",
    "        The nomear/exonerar/etc acts, one per row, with \n",
    "        same index as original text (so indices might repeat).\n",
    "    \"\"\"\n",
    "    text_segments = text_series.str.split(splitter)\n",
    "    acts = text_segments.apply(lambda text_list: text_list[1:])\n",
    "    series = acts.explode()\n",
    "    return series\n",
    "    \n",
    "\n",
    "def trim_act_tail(text_series):\n",
    "    \"\"\"\n",
    "    Remove from the end of each nomear/exonerar/etc act\n",
    "    in `text_series` (Pandas Series) the preface of the \n",
    "    following act, which is unnecessary information for the acts.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Considerar . final do ato, a menos que acompanhe números ou * em seguida ou começe com \"art\" ou \" n\".\n",
    "    sentence_end_regex = r'(?<!(?:[Rr][Tt]| [Nn]))\\.(?:[^\\d*]|$)'\n",
    "    trimmed_act = text_series.str.split(sentence_end_regex).apply(lambda arr: arr[0]) + '.'\n",
    "    \n",
    "    return trimmed_act\n",
    "\n",
    "\n",
    "def isolate_acts(text_series, act_regex):\n",
    "    \"\"\"\n",
    "    Isolate each nomear/exonerar/etc act in each row of \n",
    "    `text_series` (Pandas Series) into a different row, \n",
    "    maintaining the same indices for the acts coming from \n",
    "    the same original `text_series` row. The acts are detected \n",
    "    by starting with the `act_regex`.\n",
    "    \n",
    "    Returns a Pandas Series.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mark splitting points:\n",
    "    act_splitter = 'XXDIVXX'\n",
    "    texts_with_splitter = text_series.str.replace(act_regex, act_splitter + r'\\1', case=False)\n",
    "    # Split acts:\n",
    "    act_series = split_acts(texts_with_splitter, act_splitter)\n",
    "    \n",
    "    # Trim garbage from the end of the acts:\n",
    "    cleaned_act_series = trim_act_tail(act_series)\n",
    "    \n",
    "    return cleaned_act_series\n",
    "\n",
    "\n",
    "def remove_pattern(text_series, regex):\n",
    "    \"\"\"\n",
    "    Remove a `regex` from all rows in a `text_series`.\n",
    "    \"\"\"\n",
    "    cleaned_series = text_series.str.replace(regex, '', case=False)\n",
    "    return cleaned_series\n",
    "\n",
    "\n",
    "def remove_siape(text_series):\n",
    "    \"\"\"\n",
    "    Remove SIAPE code (and related terms) from all rows in \n",
    "    `text_series`.\n",
    "    \"\"\"\n",
    "    siape_regex = r',?\\s*?(?:(?:matr[íi]cula)?\\s*siape(?:cad)?|matr[íi]cula)\\s*?n?.?\\s*?(\\d{5,7}),?'\n",
    "    return remove_pattern(text_series, siape_regex)\n",
    "\n",
    "\n",
    "def remove_cpf(text_series):\n",
    "    \"\"\"\n",
    "    Remove CPF number (and related terms) from all rows in \n",
    "    `text_series`.\n",
    "    \"\"\"\n",
    "    cpf_regex = r',?\\s*?cpf\\s*?n?\\.?.?\\s*?([\\d.*-]{14,18}),?'\n",
    "    return remove_pattern(text_series, cpf_regex)\n",
    "\n",
    "\n",
    "def remove_no(text_series):\n",
    "    no_regex = r',?\\s*?c[oó]digo\\s*?n.? ?[\\.\\d]{5,7},?'\n",
    "    return remove_pattern(text_series, no_regex)\n",
    "\n",
    "\n",
    "def remove_processo(text_series):\n",
    "    processo_regex = r'(?:,?\\s*?conforme\\s*?|[\\s\\-.]*?)\\(?Processo\\s*?(?:SEI)?\\s*?n?.?\\s*?[\\d.\\-/]{15,20}\\)?'\n",
    "    return remove_pattern(text_series, processo_regex)\n",
    "\n",
    "\n",
    "def fix_verbs(text_series):\n",
    "    \"\"\"\n",
    "    Replace infinitive of main verbs of the acts (nomear, exonerar, etc.)\n",
    "    by present tense.\n",
    "    \"\"\"\n",
    "    clean_series = text_series.copy()\n",
    "    clean_series = clean_series.str.replace(r'nomear ?(,?)\\s*', r'Nomeia\\1 ', case=False)\n",
    "    clean_series = clean_series.str.replace(r'exonerar ?(,?)\\s*', r'Exonera\\1 ', case=False)\n",
    "    clean_series = clean_series.str.replace(r'designar ?(,?)\\s*', r'Designa\\1 ', case=False)\n",
    "    clean_series = clean_series.str.replace(r'dispensar ?(,?)\\s*', r'Dispensa\\1 ', case=False)\n",
    "    return clean_series\n",
    "\n",
    "\n",
    "def remove_preamble(text_series):\n",
    "    \"\"\"\n",
    "    Remove preamble (whose end is identified by 'resolve:')\n",
    "    from all rows in `text_series`.\n",
    "    \"\"\"\n",
    "    preamble_regex = '^.*?resolve:\\s*'\n",
    "    return remove_pattern(text_series, preamble_regex)\n",
    "\n",
    "\n",
    "def filter_low_cargos(text_series):\n",
    "    \"\"\"\n",
    "    Remove rows from `text_series` that contains low cargos \n",
    "    and, also, do not contain high cargos (all hard-coded).\n",
    "    \"\"\"\n",
    "    low_cargo_regex  = '(?:(?:das|fcp?e)[ -]*?[0123]{3}\\.[1-3]|cge[ -]+?(iii|iv|v)(?:\\W|$))'\n",
    "    high_cargo_regex = '(?:(?:das|fcp?e)[ -]*?[0123]{3}\\.[4-6]|cge[ -]+?(i|ii)(?:\\W|$))'\n",
    "    filtered = text_series.loc[~((text_series.str.contains(low_cargo_regex, case=False)) & \n",
    "                                ~(text_series.str.contains(high_cargo_regex, case=False)))]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def standardize_cargos(text_series):    \n",
    "    \"\"\"\n",
    "    Standardize and simplify parts of text in `text_series` \n",
    "    (Pandas Series) describing cargos.\n",
    "    \"\"\"\n",
    "    prefix_regex = [('DAS ',    r',?\\s*?(?:c[óo]digo)?\\s*?das[ -]*?[0123]{3}\\.([1-6]),?'), \n",
    "                    ('CA ',     r',?\\s*?(?:c[óo]digo)?\\s*?ca[ -]+?(i{1,4})(?:\\W|$),?'),\n",
    "                    ('CA-APO ', r',?\\s*?(?:c[óo]digo)?\\s*?ca-apo[ -]*?([12]),?'),\n",
    "                    ('',        r',?\\s*?(?:c[óo]digo)?\\s*?\\W(CDT)\\W,?'),\n",
    "                    ('CCD ',    r',?\\s*?(?:c[óo]digo)?\\s*?ccd[ -]+?(i{1,3})(?:\\W|$),?'),\n",
    "                    ('CGE ',    r',?\\s*?(?:c[óo]digo)?\\s*?cge[ -]+?(i{1,3})(?:\\W|$),?'),\n",
    "                    ('',        r',?\\s*?(?:c[óo]digo)?\\s*?(CPAGLO),?'),\n",
    "                    ('',        r',?\\s*?(?:c[óo]digo)?\\s*?\\W(CSP)(?:\\W|$),?'),\n",
    "                    ('',        r',?\\s*?(?:c[óo]digo)?\\s*?\\W(CSU)(?:\\W|$),?'),\n",
    "                    ('CD ',     r',?\\s*?(?:c[óo]digo)?\\s*?\\Wcd(?:[ -]*?|\\.)([123])(?:\\W|$),?'),\n",
    "                    ('',        r',?\\s*?(?:c[óo]digo)?\\s*?\\W(NE)(?:\\W|$),?'),\n",
    "                    ('CETG ',   r',?\\s*?(?:c[óo]digo)?\\s*?cetg[ -]*?(iv|v|vi|vii)(?:\\W|$),?'), \n",
    "                    ('FDS ',    r',?\\s*?(?:c[óo]digo)?\\s*?\\Wfds[ -]*?(1)(?:\\W|$),?'),\n",
    "                    ('FCPE ',   r',?\\s*?(?:c[óo]digo)?\\s*?fc?pe[ -]*?[0-9]{3}\\.([1-5]),?'),\n",
    "                    ('',        '(natureza especial)'),\n",
    "                    ('CNE ',    r',?\\s*?(?:c[óo]digo)?\\s*?cne[ -]*?([0-9]{2}),?')]\n",
    "    \n",
    "    new_text_series = text_series.copy()\n",
    "    for prefix, regex in prefix_regex:\n",
    "        new_text_series = new_text_series.str.replace(regex, ' (' + prefix + r'\\1)', case=False)\n",
    "\n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def add_label_to_df(df, orgao_label_df, lookup_col='orgao'):\n",
    "    \"\"\"\n",
    "    Modify `df` (Pandas DataFrame) in place by adding a 'label'\n",
    "    column that translates regex patterns looked for in `df` column \n",
    "    `lookup_col` (default 'orgao') to labels. The regex patterns and the \n",
    "    respective labels are stored in columns 'regex' and 'label 'from \n",
    "    `orgao_label_df` (Pandas DataFrame).\n",
    "    \"\"\"\n",
    "    df['label'] = None\n",
    "\n",
    "    for i in range(len(orgao_label)):\n",
    "\n",
    "        # Get one regex-label pair:\n",
    "        regex = orgao_label_df.loc[i, 'regex']\n",
    "        label = orgao_label_df.loc[i, 'label']\n",
    "\n",
    "        df.loc[(df[lookup_col].str.contains(regex)) & (df['label'].isnull()), 'label'] = label\n",
    "    \n",
    "    # Default:\n",
    "    df['label'].fillna('Outros')\n",
    "\n",
    "    \n",
    "def act_importance(text):\n",
    "    \"\"\"\n",
    "    Return a number representing the importance of the cargo found \n",
    "    in `text` (str).\n",
    "    \n",
    "    Note that the cargos in `text` must be standardized to the \n",
    "    hard-coded tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_importance = [('(DAS 6)', 6), ('(DAS 5)', 5), ('(DAS 4)', 4), ('(FCPE 5)', 5), ('(FCPE 4)', 4),\n",
    "                      ('(CGE I)', 5), ('(CGE II)', 4)]\n",
    "    \n",
    "    for tag, importance in tag_importance:\n",
    "        if text.find(tag) != -1:\n",
    "            return importance\n",
    "    \n",
    "    return 0   \n",
    "\n",
    "\n",
    "def remove_nomeia_cargo_preamble(text_series):\n",
    "    \"\"\"\n",
    "    Remove the preamble for a cargo/função from every \n",
    "    row in a `text_series`.\n",
    "    \n",
    "    (e.g. 'para exercer o cargo de')\n",
    "    \"\"\"\n",
    "    # Regex of the beginning:\n",
    "    enter_cargo_preamble_0 = ',?\\s*?para\\s*?(?:exercer|ocupar)\\s*?'\n",
    "    # Regex for cargo/função:\n",
    "    enter_cargo_comissao   = 'o?\\s*?cargo\\s*?(?:em\\s*?comiss[aã]o|comissionado)?'\n",
    "    enter_cargo_funcao     = 'a?\\s*?fun[cç][aã]o(?:\\s*?comissionada)?(?:\\s*?do\\s*?poder\\s*?executivo)?'\n",
    "    # Regex for the final part:\n",
    "    enter_cargo_preamble_1 = '\\s*?de'\n",
    "    # Full regex:\n",
    "    preamble_regex = enter_cargo_preamble_0 + '(?:' + enter_cargo_funcao + '|' + enter_cargo_comissao + ')' \\\n",
    "                   + enter_cargo_preamble_1\n",
    "    \n",
    "    # Remove preamble:\n",
    "    new_text_series = text_series.str.replace('(' + preamble_regex + ')', '', case=False)\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def simplify_exonera_cargo_preamble(text_series):\n",
    "    \"\"\"\n",
    "    Simplify preamble of a cargo/função in the case of \n",
    "    a exoneração/dispensa.\n",
    "    \n",
    "    (e.g.: 'do cargo comissioado de' -> 'do cargo de')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regexes:\n",
    "    exit_cargo_preamble  = '(do\\s*?cargo\\s*?(?:em\\s*?comissão|comissionado)?\\s*?de)'\n",
    "    exit_funcao_preamble = '(da\\s*?função\\s*?comissionada\\s*?(?:do\\s*?poder\\s*?executivo)?\\s*?de)'\n",
    "    \n",
    "    # Transform text series:\n",
    "    new_text_series = text_series.copy()\n",
    "    new_text_series = new_text_series.str.replace(exit_cargo_preamble, 'do cargo de', case=False)\n",
    "    new_text_series = new_text_series.str.replace(exit_funcao_preamble, 'da função de', case=False)\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def simplify_cargo_preamble(text_series):\n",
    "    \"\"\"\n",
    "    Simplify preambles like 'para exercer o cargo de' and\n",
    "    'da função comissionada do poder executivo' in `text_series`.\n",
    "    \"\"\"\n",
    "    new_text_series = text_series.copy()\n",
    "    \n",
    "    # Location of exonera/dispensa and nomeia/designa acts:\n",
    "    exonera_cases = new_text_series.str.contains('^(?:exonera|dispensa)', case=False)\n",
    "    nomeia_cases  = new_text_series.str.contains('^(?:nomeia|designa)', case=False)\n",
    "    \n",
    "    # Replace large texts for shorter ones:\n",
    "    new_text_series.loc[exonera_cases] = simplify_exonera_cargo_preamble(new_text_series.loc[exonera_cases])\n",
    "    new_text_series.loc[nomeia_cases]  = remove_nomeia_cargo_preamble(new_text_series.loc[nomeia_cases])\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def truncate_text(text, n_chars=400):\n",
    "    \"\"\"\n",
    "    If `text` (str) is longer than `n_chars` (int), return \n",
    "    the first `n_chars` characters followed by '...'; \n",
    "    otherwise, return `text`.\n",
    "    \"\"\"\n",
    "    text_len = len(text)\n",
    "    if text_len <= n_chars:\n",
    "        return text\n",
    "    else:\n",
    "        return text[:n_chars] + '...'\n",
    "    \n",
    "\n",
    "def prep_orgao_regex(name, acronym):\n",
    "    \"\"\"\n",
    "    Given a orgão `name` and its `acronym`, return a regex\n",
    "    that detects the name, possibly followed by the acronym \n",
    "    with possible variations in terms of spacing and other \n",
    "    formatting.\n",
    "    \"\"\"\n",
    "    regex = name.replace(' ', r'\\s*?') + '[\\s-]*(?:\\(?' + acronym + '\\)?)?'\n",
    "    return regex\n",
    "\n",
    "\n",
    "def name_to_sigla(text_series):\n",
    "    \"\"\"\n",
    "    Replace long reference of a orgão (name + possible acronym)\n",
    "    by its acronym in a `text_series`. All orgãos are hard-coded. \n",
    "    \"\"\"\n",
    "\n",
    "    # Hard-coded acronyms and names of órgãos:\n",
    "    sigla_list = ['FNDE', 'IBAMA', 'ICMBio', 'INCRA', 'FUNAI', 'CAPES', 'INEP', 'CNPq', 'ABIN']\n",
    "    orgao_list = ['Fundo Nacional de Desenvolvimento da Educa[cç][aã]o',\n",
    "                  'Instituto Brasileiro do Meio Ambiente e dos Recursos Naturais Renov[aá]veis',\n",
    "                  'Instituto Chico Mendes de Conserva[cç][aã]o da Biodiversidade',\n",
    "                  'Instituto Nacional de Coloniza[cç][aã]o e Reforma Agr[aá]ria',\n",
    "                  'Funda[cç][aã]o Nacional do [IÍ]ndio',\n",
    "                  'Coordena[cç][aã]o de Aperfei[cç]oamento de Pessoal de N[ií]vel Superior',\n",
    "                  'Instituto Nacional de Estudos e Pesquisas Educacionais An[ií]sio Teixeira',\n",
    "                  'Conselho Nacional de Desenvolvimento Cient[ií]fico e Tecnol[oó]gico',\n",
    "                  'Ag[eê]ncia Brasileira de Intelig[eê]ncia']\n",
    "    # Create robust regexes out of name and acronym:\n",
    "    regex_list = [prep_orgao_regex(name, acronym) for name, acronym in zip(orgao_list, sigla_list)]\n",
    "    \n",
    "    new_text_series = text_series.copy()\n",
    "    for regex, sigla in zip(regex_list, sigla_list):\n",
    "        new_text_series = new_text_series.str.replace(regex, sigla, case=False)\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def remove_dates(text_series):\n",
    "    \"\"\"\n",
    "    Remove references to dates that start with 'a partir de'\n",
    "    or 'a contar de'.\n",
    "    \"\"\"\n",
    "    # Preparing regex:\n",
    "    mes_list = ['janeiro', 'fevereiro', 'mar[cç]o', 'abril', 'maio', 'junho', 'julho', 'agosto', 'setembro', \n",
    "                'outubro', 'novembro', 'dezembro']\n",
    "    mes_regex = '(?:' + '|'.join(mes_list) + ')'\n",
    "    data_regex = r',? a (?:partir|contar) de (?:\\d{1,2}.? de ' + mes_regex + ' de (?:20|19)\\d{2}|\\d{1,2}/\\d{1,2}/\\d{4}),?'\n",
    "    data_regex = data_regex.replace(' ', '\\s*?')\n",
    "    \n",
    "    new_text_series = text_series.str.replace(data_regex, '', case=False)\n",
    "    \n",
    "    return new_text_series\n",
    "\n",
    "\n",
    "def assign_emoji(act_text):\n",
    "    \"\"\"\n",
    "    Returns a certain hard-coded emoji given a hard-coded regex \n",
    "    found in `act_text` (str).\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of regexes and emojis. The list is ordered by preference:\n",
    "    regex_emoji = [('substitu', '🎒'), (r'pol[ií]cia\\s*?(?:rodovi[aá]ria)?\\s*?federal', '👮🏻'),\n",
    "               (r'\\(DAS 6\\)', '👑'), (r'\\((?:DAS|FCPE) 5\\)', '🎩'), (r'\\((?:DAS|FCPE) 4\\)', '🧢'),  \n",
    "               (r'\\((?:CA|CGE) I{1,3}\\)', '💼'), (r'\\(CDT\\)', '👓'), \n",
    "               (r'(?:grupo de trabalho|comitê|conselho)', '💬')]\n",
    "    \n",
    "    # Look for patterns:\n",
    "    for regex, emoji in regex_emoji:\n",
    "        if re.search(regex, act_text, flags=re.IGNORECASE) != None:\n",
    "            return emoji\n",
    "    \n",
    "    # Default return:\n",
    "    return '▪️'\n",
    "\n",
    "\n",
    "def prepare_with_acts(materia_series, act_regex):\n",
    "    \"\"\"\n",
    "    Process `materia_series` (Pandas Series) of matérias from DOU that \n",
    "    contains the pattern `act_regex`. Those are assumed to be \n",
    "    standard nomeações/exonerações/designações/dispensas.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    cleaned_acts : Pandas Series\n",
    "        The individual acts present in matérias, with low cargos\n",
    "        removed and with its texts cleaned. Acts (each \n",
    "        nomeação/exoneração) in the same matéria share the same \n",
    "        index as the original matéria in `materia_series`.\n",
    "    \"\"\"\n",
    "    # Isolate each act in a different row:\n",
    "    raw_acts = isolate_acts(materia_series, act_regex)\n",
    "\n",
    "    # Filter acts containing only low cargos:\n",
    "    filtered_acts = filter_low_cargos(raw_acts)\n",
    "\n",
    "    # Remove unwanted information:\n",
    "    cleaned_acts  = filtered_acts\n",
    "    cleaned_acts  = remove_siape(cleaned_acts)\n",
    "    cleaned_acts  = remove_cpf(cleaned_acts)\n",
    "    cleaned_acts  = remove_no(cleaned_acts)\n",
    "    cleaned_acts  = remove_processo(cleaned_acts)\n",
    "    \n",
    "    # Clean text:\n",
    "    cleaned_acts  = fix_verbs(cleaned_acts)\n",
    "    cleaned_acts  = standardize_cargos(cleaned_acts)\n",
    "    cleaned_acts  = simplify_cargo_preamble(cleaned_acts)\n",
    "    cleaned_acts  = name_to_sigla(cleaned_acts)\n",
    "    cleaned_acts  = remove_dates(cleaned_acts)\n",
    "    \n",
    "    return cleaned_acts\n",
    "\n",
    "\n",
    "def prepare_no_acts(materia_series):\n",
    "    \"\"\"\n",
    "    Clean `materia_series` (Pandas Series) of matérias from DOU \n",
    "    that do not contain typical verbs of nomeação/exoneração, etc.\n",
    "    \"\"\"\n",
    "    cleaned_non_acts = materia_series\n",
    "    cleaned_non_acts = remove_preamble(cleaned_non_acts)\n",
    "    cleaned_non_acts = cleaned_non_acts.apply(truncate_text)\n",
    "    \n",
    "    return cleaned_non_acts\n",
    "\n",
    "\n",
    "def sort_orgaos_by_acts_importance(message_df, orgao_importance):\n",
    "    \"\"\"\n",
    "    Define the order of the orgãos in the message, according to \n",
    "    the relevance of the acts.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    message_df : Pandas DataFrame\n",
    "        A DataFrame containing the section in which the acts \n",
    "        will be published and their importance.\n",
    "        \n",
    "    orgao_importance : Pandas Series\n",
    "        A series whose indices are the sections (labels for orgãos)\n",
    "        and the values are the label's importances, for breaking \n",
    "        ties.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    ordered_sections : array\n",
    "        The orgãos labels sorted as they should appear in the message.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute stats for each orgao (message's sections):\n",
    "    sections_groupby_importance = message_df.groupby('section')['importance']\n",
    "    sections_importance_sum = sections_groupby_importance.sum()\n",
    "    sections_importance_max = sections_groupby_importance.max()\n",
    "    \n",
    "    # Build DataFrame with the section's stats:\n",
    "    sections_ranking_df = pd.DataFrame()\n",
    "    sections_ranking_df['max'] = sections_importance_max\n",
    "    sections_ranking_df['sum'] = sections_importance_sum\n",
    "    sections_ranking_df = sections_ranking_df.join(orgao_importance, how='left')\n",
    "\n",
    "    # Order the sections:\n",
    "    ordered_sections = sections_ranking_df.sort_values(['max','sum', 'importance'], ascending=False).index.values\n",
    "    \n",
    "    return ordered_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading orgão-label table...\n",
      "Loading data from BigQuery...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 9/9 [00:01<00:00,  6.69rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to local file...\n",
      "# matérias (all): 9\n",
      "Processing the matérias...\n",
      "# matérias (containing act verbs): 8\n",
      "# matérias (without act verbs): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### MAIN CODE ###\n",
    "\n",
    "\n",
    "### Hard-coded stuff:\n",
    "\n",
    "orgao_label_file = 'data/correspondencia_orgao_label_DOU_2.csv'\n",
    "todays_dou_data  = 'temp/daily_ranked_dou_2_set.csv'\n",
    "zap_group_link   = 'https://chat.whatsapp.com/JUwrwHaDpnBK6WnAgMTLXV'\n",
    "#zap_group_link   = 'https://chat.whatsapp.com/GJGfNBzcZZT9xuueZUBbTU'\n",
    "post_file_prefix = 'posts/dou_2_'\n",
    "text_editor      = 'gedit'\n",
    "\n",
    "prod_query = \"\"\"\n",
    "SELECT relevancia, identifica, secao, edicao, data_pub, orgao, ementa, resumo, fulltext, assina, cargo, url \n",
    "FROM `gabinete-compartilhado.executivo_federal_dou.sheets_classificacao_secao_2`\n",
    "WHERE relevancia IS NOT NULL\n",
    "AND   relevancia >= 3\n",
    "\"\"\"\n",
    "\n",
    "### Load data:\n",
    "\n",
    "# Table that translates orgao to message topic:\n",
    "print('Loading orgão-label table...')\n",
    "orgao_label = pd.read_csv(orgao_label_file)\n",
    "\n",
    "# Download today's ranked DOU (section 2) materias:\n",
    "test_set = load_data_from_local_or_bigquery(prod_query, todays_dou_data, force_bigquery=True)\n",
    "print('# matérias (all):', len(test_set))\n",
    "\n",
    "\n",
    "### Prepare the data:\n",
    "\n",
    "print('Processing the matérias...')\n",
    "\n",
    "# Add label tag to all texts:\n",
    "add_label_to_df(test_set, orgao_label)\n",
    "\n",
    "# Use regex to detect typical act verbs:\n",
    "enter_regex   = r'nomear|designar'\n",
    "exit_regex    = r'exonerar|dispensar'\n",
    "flexing_regex = r'(?!(?:am|á|ão|em))'\n",
    "act_regex     = r'(' + enter_regex + '|' + exit_regex + ')' + flexing_regex\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    # Get articles with default act detectors:\n",
    "    with_act_regex_df = test_set.loc[test_set.fulltext.str.contains(act_regex, case=False)]\n",
    "    print('# matérias (containing act verbs):', len(with_act_regex_df))\n",
    "    # Get articles without default act detectors:\n",
    "    no_act_regex_df   = test_set.loc[~test_set.fulltext.str.contains(act_regex, case=False)]\n",
    "    print('# matérias (without act verbs):', len(no_act_regex_df))\n",
    "\n",
    "    # Clean acts for posting:\n",
    "    cleaned_with_acts = prepare_with_acts(with_act_regex_df['fulltext'], act_regex)\n",
    "    cleaned_no_acts   = prepare_no_acts(no_act_regex_df['fulltext'])\n",
    "    # Join both kinds of matérias:\n",
    "    cleaned_all = pd.concat([cleaned_with_acts, cleaned_no_acts], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the post...\n",
      "Writing post...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prepare the message:\n",
    "\n",
    "print('Preparing the post...')\n",
    "\n",
    "# Build zap message DataFrame:\n",
    "message_df = pd.DataFrame()\n",
    "message_df['text']       = cleaned_all\n",
    "message_df['importance'] = cleaned_all.apply(act_importance)\n",
    "message_df['section']    = test_set['label'][cleaned_all.index]\n",
    "message_df['url']        = test_set['url'][cleaned_all.index]\n",
    "message_df = message_df.reset_index(drop=True)\n",
    "\n",
    "# Prepare the order in which the orgãos will appear in the message:\n",
    "ordered_sections = sort_orgaos_by_acts_importance(message_df, orgao_label.set_index('label')['importance'])\n",
    "\n",
    "\n",
    "### Print the message:\n",
    "\n",
    "print('Writing post...')\n",
    "\n",
    "filename = post_file_prefix + date.today().strftime('%Y-%m-%d') + '.txt'\n",
    "with open(filename, 'w') as f:\n",
    "\n",
    "    # Header:\n",
    "    today = date.today().strftime(' (%d/%m)')\n",
    "    f.write('♟️ *Alterações em cargos altos' + today + '* ♟️\\n\\n')\n",
    "\n",
    "    # Loop over orgãos:\n",
    "    for s in ordered_sections:\n",
    "        f.write('*' + s + '*\\n\\n')\n",
    "\n",
    "        # Select acts from this section:\n",
    "        section_acts = message_df.loc[message_df['section'] == s].sort_values('importance', ascending=False)\n",
    "        for t, u in zip(section_acts['text'].values, section_acts['url'].values):\n",
    "            # Print message:\n",
    "            e = assign_emoji(t)\n",
    "            f.write(e + ' ' + t + '\\n' + u + '\\n\\n')\n",
    "\n",
    "            # Footnote:        \n",
    "    f.write('*Gabinete Compartilhado Acredito*\\n_Para se inscrever no boletim, acesse o link:_\\n' + zap_group_link)\n",
    "\n",
    "\n",
    "### Open text editor:\n",
    "\n",
    "subprocess.call([text_editor, filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
